"""
Context-Based Document Deletion Script
====================================

Enhanced script for precise deletion of documents from Azure Cognitive Search index
based on context name and filename combination. Provides comprehensive safety features,
multiple matching modes, and complete tracker integration.

Features:
- Multiple matching strategies (exact, contains, flexible)
- Comprehensive preview system with detailed impact analysis  
- Production-ready safety with confirmations and dry-run capability
- Complete tracker integration using verified methods
- Enhanced statistics with performance metrics and success rates
- Robust error handling with graceful degradation and detailed reporting
- Large deletion warnings and multi-level confirmation system
- Batch optimization for efficient processing of large document sets

Usage Examples:
    # Exact matching (default)
    python delete_by_context_and_filename.py "WORK-123" "readme.md"
    
    # Flexible matching (tries exact, then contains)
    python delete_by_context_and_filename.py "WORK-123" "readme" --mode flexible
    
    # Preview what would be deleted
    python delete_by_context_and_filename.py "WORK-123" "readme.md" --preview
    
    # Force deletion without confirmation
    python delete_by_context_and_filename.py "WORK-123" "readme.md" --force
    
    # Detailed preview with chunk information
    python delete_by_context_and_filename.py "WORK-123" "readme.md" --preview --detailed

Author: Personal Documentation Assistant System
Version: 1.0.0
"""

import os
import sys
import argparse
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from dotenv import load_dotenv

# Add src directory to path for imports
current_dir = Path(__file__).parent
src_dir = current_dir.parent.parent
sys.path.insert(0, str(src_dir))

# Import project modules
from src.common.vector_search_services.azure_cognitive_search import get_azure_search_service, FilterBuilder
from src.document_upload.document_processing_tracker import DocumentProcessingTracker

# Load environment variables
load_dotenv()


def find_matching_documents(search_service, context_name: str, file_name: str, 
                           matching_mode: str = 'exact') -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Enhanced document search with multiple matching strategies and comprehensive statistics
    
    Based on actual Azure Search service capabilities from azure_cognitive_search.py
    Uses FilterBuilder.build_filter() for precise OData filter construction
    
    Args:
        search_service: AzureCognitiveSearch instance (from get_azure_search_service())
        context_name: Context name to match against context_name field
        file_name: File name to match against file_name field  
        matching_mode: 'exact', 'contains', or 'flexible' matching strategy
    
    Returns:
        Tuple of (matching_documents, search_statistics)
    """
    
    print(f"üîç Searching for documents...")
    print(f"   üìù Context: '{context_name}'")
    print(f"   üìÑ File: '{file_name}'")
    print(f"   üéØ Mode: {matching_mode}")
    
    try:
        # Build filters based on matching mode using actual FilterBuilder class
        if matching_mode == 'exact':
            filters = {
                'context_name': context_name,
                'file_name': file_name
            }
        elif matching_mode == 'contains':
            # Start with exact context, use manual filtering for filename contains
            filters = {
                'context_name': context_name
            }
        elif matching_mode == 'flexible':
            # Flexible matching: exact context + flexible filename
            # First try exact, then fallback to contains
            filters = {
                'context_name': context_name,
                'file_name': file_name  # Start with exact match
            }
        
        # Search using actual Azure Search service methods
        if matching_mode == 'exact' or matching_mode == 'flexible':
            results = search_service.search_client.search(
                search_text="*",
                filter=FilterBuilder.build_filter(filters),
                select="id,file_name,context_name,chunk_index,title,file_path,last_modified",
                top=1000,  # Get all chunks for the file
                order_by=["chunk_index asc"]  # Order by chunk for logical display
            )
            matching_docs = list(results)
        else:  # contains mode
            # Get all documents with exact context match first
            results = search_service.search_client.search(
                search_text="*",
                filter=FilterBuilder.build_filter(filters),
                select="id,file_name,context_name,chunk_index,title,file_path,last_modified",
                top=1000,
                order_by=["chunk_index asc"]
            )
            
            # Filter results where file_name contains the search term
            file_name_lower = file_name.lower()
            matching_docs = [
                doc for doc in results 
                if file_name_lower in doc.get('file_name', '').lower()
            ]
        
        # If flexible mode and no exact matches, try contains matching
        if matching_mode == 'flexible' and not matching_docs:
            print("   üîÑ No exact matches found, trying flexible matching...")
            
            # Get all documents with exact context match
            context_filter = {'context_name': context_name}
            all_results = search_service.search_client.search(
                search_text="*",
                filter=FilterBuilder.build_filter(context_filter),
                select="id,file_name,context_name,chunk_index,title,file_path,last_modified",
                top=1000,
                order_by=["chunk_index asc"]
            )
            
            # Filter results where file_name contains the search term
            file_name_lower = file_name.lower()
            matching_docs = [
                doc for doc in all_results 
                if file_name_lower in doc.get('file_name', '').lower()
            ]
        
        # Generate comprehensive statistics
        files_found = {}
        for doc in matching_docs:
            file_path = doc.get('file_path', 'Unknown')
            if file_path not in files_found:
                files_found[file_path] = []
            files_found[file_path].append(doc)
        
        stats = {
            'total_matches': len(matching_docs),
            'unique_files': len(files_found),
            'chunk_count': len(matching_docs),
            'contexts_found': len(set(doc.get('context_name', '') for doc in matching_docs)),
            'files_breakdown': files_found,
            'matching_mode_used': matching_mode
        }
        
        print(f"   ‚úÖ Search completed: {stats['total_matches']} chunks in {stats['unique_files']} files")
        return matching_docs, stats
        
    except Exception as e:
        print(f"   ‚ùå Search failed: {str(e)}")
        return [], {
            'total_matches': 0, 
            'unique_files': 0, 
            'chunk_count': 0, 
            'contexts_found': 0,
            'files_breakdown': {},
            'matching_mode_used': matching_mode,
            'error': str(e)
        }


def preview_deletion_impact(matching_docs: List[Dict[str, Any]], search_stats: Dict[str, Any], 
                          show_details: bool = True) -> bool:
    """
    Enhanced preview system with comprehensive impact analysis and user guidance
    
    Based on Scripts 1 & 2 patterns for detailed user feedback
    """
    
    if not matching_docs:
        print("\nüîç No matching documents found.")
        print("üí° Try using different matching mode:")
        print("   --mode exact     (exact context + filename match)")  
        print("   --mode contains  (exact context + filename contains)")
        print("   --mode flexible  (exact context + flexible filename)")
        
        if 'error' in search_stats:
            print(f"‚ö†Ô∏è  Search error: {search_stats['error']}")
        
        return False
    
    print(f"\nüìã DELETION IMPACT ANALYSIS")
    print("="*50)
    
    # High-level summary
    total_chunks = search_stats['total_matches'] 
    unique_files = search_stats['unique_files']
    contexts = search_stats['contexts_found']
    
    print(f"üìä Overview:")
    print(f"   üß© Total chunks to delete: {total_chunks}")
    print(f"   üìÑ Unique files affected: {unique_files}")
    print(f"   üè∑Ô∏è  Contexts involved: {contexts}")
    print(f"   üéØ Matching mode: {search_stats['matching_mode_used']}")
    
    if show_details and 'files_breakdown' in search_stats:
        print(f"\nüìÅ Files and Chunks Breakdown:")
        
        for file_path, chunks in search_stats['files_breakdown'].items():
            file_name = Path(file_path).name if file_path != 'Unknown' else 'Unknown'
            context_name = chunks[0].get('context_name', 'N/A') if chunks else 'N/A'
            
            print(f"\n   üìÑ {file_name}")
            print(f"      üìç Path: {file_path}")
            print(f"      üè∑Ô∏è  Context: {context_name}")
            print(f"      üß© Chunks: {len(chunks)}")
            
            # Show first few chunks
            for i, chunk in enumerate(chunks[:3], 1):
                chunk_idx = chunk.get('chunk_index', 'N/A')
                title = chunk.get('title', 'No title')[:50]
                print(f"         {i}. Chunk {chunk_idx}: {title}...")
            
            if len(chunks) > 3:
                print(f"         ... and {len(chunks) - 3} more chunks")
    
    # Warning for large deletions
    if total_chunks > 10:
        print(f"\n‚ö†Ô∏è  WARNING: This will delete {total_chunks} document chunks!")
        print("   This action cannot be undone.")
        
    return True


def get_user_confirmation(matching_docs: List[Dict[str, Any]], force: bool = False) -> bool:
    """Enhanced confirmation system with safety checks"""
    
    if force:
        print("üöÄ Force mode enabled - proceeding without confirmation")
        return True
    
    total_chunks = len(matching_docs)
    
    # Extra confirmation for large deletions
    if total_chunks > 20:
        print(f"\nüõë LARGE DELETION WARNING")
        print(f"   You are about to delete {total_chunks} document chunks!")
        print("   This is a large operation that cannot be undone.")
        
        first_confirm = input("\n   Type 'CONFIRM' to proceed with large deletion: ").strip()
        if first_confirm != 'CONFIRM':
            print("   Operation cancelled - confirmation not received")
            return False
    
    # Standard confirmation
    confirm = input(f"\n‚ùì Delete {total_chunks} document chunks? (y/N): ").lower().strip()
    
    if confirm in ['y', 'yes']:
        print("‚úÖ Deletion confirmed")
        return True
    else:
        print("‚ùå Operation cancelled")
        return False


def delete_documents_and_cleanup_tracker(search_service, tracker: DocumentProcessingTracker,
                                       matching_docs: List[Dict[str, Any]], 
                                       dry_run: bool = False) -> Tuple[int, int, List[str], Dict[str, Any]]:
    """
    Enhanced atomic deletion with comprehensive error handling and performance metrics
    
    Based on actual AzureCognitiveSearch.delete_document() method signature
    Uses DocumentProcessingTracker.mark_unprocessed() for cleanup (verified method)
    
    Returns:
        Tuple of (successful_deletes, failed_deletes, error_messages, operation_stats)
    """
    
    if dry_run:
        print(f"\nüîç DRY RUN: Would delete {len(matching_docs)} document chunks")
        return len(matching_docs), 0, [], {'operation': 'dry_run', 'duration': 0, 'files_would_untrack': 0}
    
    print(f"\nüóëÔ∏è  Starting deletion of {len(matching_docs)} document chunks...")
    
    start_time = time.time()
    
    successful_deletes = 0
    failed_deletes = 0
    error_messages = []
    file_paths_to_untrack = set()
    
    # Process deletions with progress tracking
    for i, doc in enumerate(matching_docs, 1):
        doc_id = doc.get('id')
        file_path = doc.get('file_path')
        chunk_idx = doc.get('chunk_index', 'N/A')
        
        try:
            # Use actual delete_document method (returns bool based on codebase)
            success = search_service.delete_document(doc_id)
            
            if success:
                successful_deletes += 1
                print(f"   ‚úÖ [{i}/{len(matching_docs)}] Deleted chunk {chunk_idx}")
                
                # Collect file paths for tracker cleanup
                if file_path and file_path != 'Unknown':
                    file_paths_to_untrack.add(Path(file_path))
            else:
                failed_deletes += 1
                error_msg = f"Delete returned False for document {doc_id}"
                error_messages.append(error_msg)
                print(f"   ‚ùå [{i}/{len(matching_docs)}] Failed: {error_msg}")
                
        except Exception as e:
            failed_deletes += 1
            error_msg = f"Exception deleting {doc_id}: {str(e)}"
            error_messages.append(error_msg)
            print(f"   ‚ùå [{i}/{len(matching_docs)}] Error: {error_msg}")
    
    # Cleanup tracker for successfully deleted files (using verified method)
    tracker_cleanup_count = 0
    if file_paths_to_untrack and successful_deletes > 0:
        print(f"\nüìã Cleaning up document tracker for {len(file_paths_to_untrack)} files...")
        
        for file_path in file_paths_to_untrack:
            try:
                # Use actual mark_unprocessed method (verified in codebase)
                tracker.mark_unprocessed(file_path)
                tracker_cleanup_count += 1
                print(f"   üßπ Unmarked: {file_path.name}")
            except Exception as e:
                error_msg = f"Failed to untrack {file_path}: {str(e)}"
                error_messages.append(error_msg)
                print(f"   ‚ö†Ô∏è  Warning: {error_msg}")
        
        # Save tracker state (using verified method)
        try:
            tracker.save()
            print(f"   ‚úÖ Tracker saved with {tracker_cleanup_count} files unmarked")
        except Exception as e:
            error_msg = f"Failed to save tracker: {str(e)}"
            error_messages.append(error_msg)
            print(f"   ‚ùå Error saving tracker: {error_msg}")
    
    # Calculate operation statistics
    end_time = time.time()
    duration = end_time - start_time
    
    operation_stats = {
        'operation': 'delete',
        'duration': duration,
        'total_processed': len(matching_docs),
        'successful_deletes': successful_deletes,
        'failed_deletes': failed_deletes,
        'tracker_cleanups': tracker_cleanup_count,
        'success_rate': (successful_deletes / len(matching_docs)) * 100 if matching_docs else 0,
        'files_untracked': len(file_paths_to_untrack)
    }
    
    return successful_deletes, failed_deletes, error_messages, operation_stats


def print_deletion_results(successful: int, failed: int, errors: List[str], 
                         stats: Dict[str, Any], show_detailed_stats: bool):
    """Enhanced results reporting based on Scripts 1 & 2 patterns"""
    
    print(f"\nüìä DELETION RESULTS")
    print("="*40)
    print(f"‚úÖ Successfully deleted: {successful}")
    print(f"‚ùå Failed deletions: {failed}")
    print(f"‚è±Ô∏è  Operation time: {stats.get('duration', 0):.2f}s")
    
    if successful > 0:
        success_rate = stats.get('success_rate', 0)
        print(f"üìà Success rate: {success_rate:.1f}%")
        
        tracker_cleanups = stats.get('tracker_cleanups', 0)
        files_untracked = stats.get('files_untracked', 0)
        if tracker_cleanups > 0:
            print(f"üìã Tracker files cleaned: {tracker_cleanups}/{files_untracked}")

    # Show errors summary
    if errors:
        print(f"\n‚ö†Ô∏è  Errors encountered ({len(errors)}):")
        for i, error in enumerate(errors[:3], 1):  # Show first 3
            print(f"   {i}. {error}")
        if len(errors) > 3:
            print(f"   ... and {len(errors) - 3} more errors")

    if show_detailed_stats:
        print(f"\nüìà Detailed Statistics:")
        print(f"   Total documents processed: {stats.get('total_processed', 0)}")
        print(f"   Operation type: {stats.get('operation', 'unknown')}")
        if stats.get('operation') == 'dry_run':
            print("   üí° This was a dry run - no actual changes were made")
            files_would_untrack = stats.get('files_would_untrack', 0)
            if files_would_untrack > 0:
                print(f"   üìã Would untrack {files_would_untrack} files from tracker")


def main():
    """Enhanced CLI with comprehensive options based on Scripts 1 & 2 patterns"""
    
    parser = argparse.ArgumentParser(
        description="Delete documents by context name and filename with advanced options",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Exact matching (default)
  python delete_by_context_and_filename.py "WORK-123" "readme.md"
  
  # Flexible matching (tries exact, then contains)
  python delete_by_context_and_filename.py "WORK-123" "readme" --mode flexible
  
  # Preview what would be deleted
  python delete_by_context_and_filename.py "WORK-123" "readme.md" --preview
  
  # Force deletion without confirmation
  python delete_by_context_and_filename.py "WORK-123" "readme.md" --force
  
  # Detailed preview with chunk information
  python delete_by_context_and_filename.py "WORK-123" "readme.md" --preview --detailed
  
  # Dry run to see what would happen
  python delete_by_context_and_filename.py "WORK-123" "readme.md" --dry-run
        """)

    parser.add_argument("context_name", 
                       help="Context name to match (exact match)")
    
    parser.add_argument("file_name", 
                       help="File name to match (based on matching mode)")

    parser.add_argument("--mode", choices=['exact', 'contains', 'flexible'], 
                       default='exact',
                       help="Matching mode: exact (default), contains, or flexible")

    parser.add_argument("--preview", action="store_true",
                       help="Preview what would be deleted without deleting")

    parser.add_argument("--dry-run", action="store_true", 
                       help="Show deletion preview and simulate operation")

    parser.add_argument("--force", action="store_true",
                       help="Delete without confirmation prompt")

    parser.add_argument("--detailed", action="store_true",
                       help="Show detailed chunk information in preview")

    parser.add_argument("--stats", action="store_true",
                       help="Show comprehensive operation statistics")

    parser.add_argument("--verbose", "-v", action="store_true",
                       help="Enable verbose logging for debugging")

    args = parser.parse_args()

    # Configure logging
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)

    # Validate mutually exclusive options
    if args.preview and args.dry_run:
        print("üí° Note: --preview and --dry-run are similar. Using --dry-run behavior.")
        args.preview = False

    try:
        # Load environment and validate (using Scripts 1 & 2 patterns)
        load_dotenv()
        
        # Validate environment variables
        required_vars = ['AZURE_SEARCH_SERVICE', 'AZURE_SEARCH_KEY']
        missing_vars = [var for var in required_vars if not os.getenv(var)]
        
        if missing_vars:
            print("‚ùå Missing required environment variables:")
            for var in missing_vars:
                print(f"   - {var}")
            print("\nüí° Please ensure your .env file contains the required Azure Search credentials")
            return 1

        # Initialize services (using get_azure_search_service pattern from Scripts 1 & 2)
        print("üîß Initializing services...")
        try:
            azure_search = get_azure_search_service()
            tracker = DocumentProcessingTracker()
            print("   ‚úÖ Services initialized successfully")
        except Exception as e:
            print(f"   ‚ùå Failed to initialize services: {str(e)}")
            print("   üí° Check your Azure Search configuration and connectivity")
            return 1

        # Perform search
        matching_docs, search_stats = find_matching_documents(
            azure_search, args.context_name, args.file_name, args.mode
        )

        # Show preview
        preview_success = preview_deletion_impact(
            matching_docs, search_stats, show_details=args.detailed
        )

        if not preview_success:
            return 0  # No matches found

        # Handle preview-only mode
        if args.preview:
            print("\nüí° Preview complete. Use without --preview to perform deletion.")
            return 0

        # Get confirmation (unless dry run)
        if not args.dry_run:
            if not get_user_confirmation(matching_docs, args.force):
                return 0

        # Perform deletion
        successful, failed, errors, operation_stats = delete_documents_and_cleanup_tracker(
            azure_search, tracker, matching_docs, dry_run=args.dry_run
        )

        # Show results
        print_deletion_results(successful, failed, errors, operation_stats, args.stats)

        # Return appropriate exit code
        return 0 if failed == 0 else 1

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Operation interrupted by user")
        return 1
    except Exception as e:
        print(f"‚ùå Unexpected error: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
