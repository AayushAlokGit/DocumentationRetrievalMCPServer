# Document Upload System Setup Guide ğŸ“„

Complete setup guide for the **Document Processing & Upload System** component.

## ğŸ¯ What This Component Does

The Document Upload System processes your documentation files and creates a searchable index in Azure Cognitive Search. This is the **foundation** that enables the MCP server to provide intelligent search capabilities.

**You need to complete this setup FIRST** before using the MCP server.

## ğŸ“‹ Prerequisites

### Azure Services Required

- **Azure OpenAI Service** with text-embedding-ada-002 deployment
- **Azure Cognitive Search** service (Basic tier or higher for vector search)

### Local Requirements

- **Python 3.8+**
- **Documentation directory** with files organized as:
  ```
  Documentation/
  â”œâ”€â”€ Project-A/
  â”‚   â”œâ”€â”€ requirements.md
  â”‚   â””â”€â”€ implementation.md
  â””â”€â”€ Research-B/
      â””â”€â”€ analysis.md
  ```

## ğŸš€ Step-by-Step Setup

### Step 1: Azure OpenAI Service Setup

1. **Create Azure OpenAI Resource:**

   - Go to [Azure Portal](https://portal.azure.com)
   - Create new resource â†’ AI + Machine Learning â†’ Azure OpenAI
   - Choose region (e.g., East US, West Europe)

2. **Deploy Embedding Model:**

   - Navigate to Azure OpenAI Studio
   - Go to Deployments â†’ Create new deployment
   - Deploy **text-embedding-ada-002** model
   - Note down the deployment name

3. **Get Credentials:**
   - Copy the **Endpoint URL** (e.g., `https://your-service.openai.azure.com/`)
   - Copy one of the **API keys**

### Step 2: Azure Cognitive Search Setup

1. **Create Search Service:**

   - Go to Azure Portal
   - Create new resource â†’ Web â†’ Azure Cognitive Search
   - **Important**: Choose **Basic tier or higher** (Free tier doesn't support vector search)
   - Choose same region as OpenAI service for better performance

2. **Get Credentials:**
   - Note down the **Service name** (e.g., `my-search-service`)
   - Go to Keys section â†’ Copy the **Primary admin key**

### Step 3: Environment Configuration

1. **Navigate to project directory:**

   ```bash
   cd DocumentationRetrievalMCPServer
   ```

2. **Create virtual environment:**

   ```bash
   python -m venv venv

   # Activate (Windows)
   venv\Scripts\activate

   # Activate (macOS/Linux)
   source venv/bin/activate
   ```

3. **Install dependencies:**

   ```bash
   pip install -r requirements.txt
   ```

4. **Create environment file:**

   ```bash
   cp .env.example .env
   ```

5. **Configure .env file:**
   Edit `.env` with your Azure credentials:

   ```env
   # Azure OpenAI Configuration
   AZURE_OPENAI_ENDPOINT=https://your-openai-service.openai.azure.com/
   AZURE_OPENAI_KEY=your-openai-api-key-here
   EMBEDDING_DEPLOYMENT=text-embedding-ada-002
   OPENAI_API_VERSION=2024-05-01-preview

   # Azure Cognitive Search Configuration
   AZURE_SEARCH_SERVICE=your-search-service-name
   AZURE_SEARCH_KEY=your-search-admin-key-here
   AZURE_SEARCH_INDEX=documentation-index

   # Local Paths - UPDATE THIS PATH
   PERSONAL_DOCUMENTATION_ROOT_DIRECTORY=C:\path\to\your\Documentation

   # Processing Configuration (Optional)
   CHUNK_SIZE=1000
   CHUNK_OVERLAP=200
   VECTOR_DIMENSIONS=1536
   ```

### Step 4: Verify Document Upload Setup

Run the comprehensive verification script to validate all system components:

```bash
python src\document_upload\common_scripts\verify_document_upload_setup.py
```

This script performs **comprehensive system validation** including:

- ğŸ” **Environment Variables**: Validates all required Azure credentials and paths
- ğŸ¤– **Azure OpenAI Connection**: Tests embedding service and model deployment
- ğŸ” **Azure Cognitive Search**: Verifies service connectivity, index existence, and document counts
- ğŸ“ **Documentation Directory**: Scans for context folders and supported documents (.md, .txt, .docx)
- ğŸ§ª **Pipeline Components**: Tests import of processing strategies and document handling
- âš¡ **End-to-End Test**: Validates complete document processing pipeline

Expected output includes:

```
ğŸ” Environment Variables
âœ… Environment file exists                Found: .env
âœ… AZURE_OPENAI_ENDPOINT                 https://your-service.openai.azure.com/
âœ… AZURE_OPENAI_KEY                      Configured (32 chars)
âœ… AZURE_SEARCH_SERVICE                  your-search-service
âœ… AZURE_SEARCH_KEY                      Configured (32 chars)
âœ… PERSONAL_DOCUMENTATION_ROOT_DIRECTORY C:\path\to\Documentation

ğŸ¤– Azure OpenAI (Embeddings)
âœ… Azure OpenAI connection               Service accessible
âœ… Embedding service initialization      Using deployment: text-embedding-ada-002
âœ… Test embedding generation             Generated 1536-dimensional vector

ğŸ” Azure Cognitive Search
âœ… Azure Cognitive Search connection     Service accessible, found 2 indexes
âœ… Azure Search Index exists             Index 'documentation-index' found
âœ… Index document count                  Index contains 234 documents

ğŸ“ Documentation Directory Structure
âœ… Documentation directory exists        Found: C:\path\to\Documentation
âœ… Context directories found             Found 12 context directories
âœ… Documents found                       Found 145 documents in first 10 contexts

ğŸ“Š Verification Summary
âœ… Environment: 6/6 tests passed
âœ… Azure: 5/5 tests passed
âœ… Directory: 3/3 tests passed
âœ… Pipeline: 4/4 tests passed
âœ… Test: 1/1 tests passed

ğŸ“Š Overall: 19/19 tests passed
ğŸ‰ All verification tests passed! Your system is ready for document upload.

ğŸš€ SYSTEM READY: You can now run document upload scripts!
ğŸ“‹ Next steps:
   1. Run: python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --dry-run
   2. Run: python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Project-A"
   3. Force reprocess: python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Project-A" --force
```

**ğŸ”§ System Readiness Assessment:**
The script evaluates both **critical requirements** (must pass for system to function) and **optional features** (nice-to-have enhancements).

### Step 5: Create Search Index

Create the Azure Cognitive Search index with vector search capabilities:

```bash
python src\document_upload\common_scripts\create_index.py
```

This script:

- **Creates index** with name from `AZURE_SEARCH_INDEX` environment variable
- **Configures vector search** with 1536-dimensional embeddings (matching OpenAI ada-002)
- **Checks existence** and safely handles index recreation if needed
- **Validates configuration** against your Azure Cognitive Search service tier

Expected output:

```
ğŸ”§ Creating Azure Cognitive Search index: documentation-index
âœ… Index created successfully with vector search capabilities
ğŸ“Š Index configuration:
   â€¢ Vector dimensions: 1536 (OpenAI ada-002 compatible)
   â€¢ Search fields: content, file_path, context_name, title
   â€¢ Vector field: content_vector
   â€¢ Hybrid search enabled
```

### Step 6: Upload Your Documents

The main upload script provides comprehensive document processing with multiple operation modes:

```bash
# Preview what will be processed (recommended first step)
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --dry-run
```

**Available Command Options:**

```bash
# Basic Operations
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py
# Processes all contexts, skipping unchanged files (using file signature tracking)

python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Project-A"
# Process only specific context (supports partial matching)

python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --dry-run
# Preview mode: shows what would be processed without making changes

# Force Processing Options
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --force --context "Project-A"
# Force reprocessing of specific context (deletes existing + re-uploads)

python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --reset
# Full system reset: clears tracking data + deletes all indexed documents + reprocesses everything
```

**Expected Output (Normal Processing):**

```
[TRACKER] Initialized DocumentProcessingTracker with: C:\Documentation\.processed_files.json
ğŸ“ Scanning documentation directory: C:\Documentation
ğŸ“ Discovered 12 context directories
ğŸ“„ Found 45 documents (.md, .txt, .docx files)

ğŸ”„ Starting document processing pipeline...
ğŸ“Š Processing batch 1/3 (15 files)
âœ… Project-A\requirements.md â†’ 3 chunks processed
âœ… Project-A\implementation.md â†’ 5 chunks processed
â­ï¸  Project-A\notes.md â†’ skipped (no changes detected)

ğŸ“Š Processing Summary:
   â€¢ Contexts Processed: 12
   â€¢ Files Discovered: 45
   â€¢ Files Processed: 30 (new/changed)
   â€¢ Files Skipped: 15 (unchanged)
   â€¢ Document Chunks Created: 234
   â€¢ Processing Time: 45.2 seconds
   â€¢ Index: documentation-index
```

**Expected Output (Dry Run):**

```
ğŸƒ DRY RUN MODE - No actual processing will occur

ğŸ“ Would process contexts: Project-A, Research-B, API-Docs
ğŸ“„ Would process 45 files (30 new/changed, 15 unchanged)
âš¡ Would generate approximately 234 document chunks
ğŸ“‹ Would update index: documentation-index

ğŸ’¡ To proceed with actual processing, remove --dry-run flag
```

## ğŸ§ª Testing Your Upload

### Test 1: Quick Document Count Check

```bash
python -c "
import sys; sys.path.append('src')
from common.azure_cognitive_search import AzureCognitiveSearch
search_svc = AzureCognitiveSearch()
count = search_svc.get_document_count()
contexts = search_svc.get_contexts()
print(f'ğŸ“Š Documents indexed: {count}')
print(f'ğŸ“‹ Contexts: {len(contexts)}')
print(f'ğŸ·ï¸  Context Names: {list(contexts)[:5]}...')
"
```

### Test 2: Search Functionality Validation

```bash
python -c "
import sys; sys.path.append('src')
from mcp_server.tools.universal_tools import UniversalDocumentationTools
import asyncio

async def test_search():
    tools = UniversalDocumentationTools()

    # Test text search
    text_results = await tools.search_documents('authentication', 'text', max_results=3)
    print(f'ğŸ” Text search results: {len(text_results)} for \"authentication\"')

    # Test vector search
    vector_results = await tools.search_documents('security requirements', 'vector', max_results=3)
    print(f'ğŸ§  Vector search results: {len(vector_results)} for \"security requirements\"')

    # Test hybrid search
    hybrid_results = await tools.search_documents('API documentation', 'hybrid', max_results=3)
    print(f'âš¡ Hybrid search results: {len(hybrid_results)} for \"API documentation\"')

asyncio.run(test_search())
"
```

### Test 3: Comprehensive System Validation

Re-run the verification script to confirm everything is working:

```bash
python src\document_upload\common_scripts\verify_document_upload_setup.py
```

Look for the final status: **ğŸš€ SYSTEM READY: You can now run document upload scripts!**

## ğŸ”§ Available Scripts & Commands

The document upload system provides a comprehensive set of utilities for different workflows:

### ğŸ“„ Document Upload & Processing

**Main Upload Script** (Primary Interface):

```bash
# Upload all contexts (processes only new/changed files)
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py

# Preview changes before processing
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --dry-run

# Process specific context
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Project-A"

# Force reprocessing (deletes existing documents for context + re-uploads)
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Project-A" --force

# Complete system reset (clears all tracking + deletes all documents + reprocesses everything)
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --reset
```

**Alternative Upload Scripts**:

```bash
# Upload with custom metadata
python src\document_upload\personal_documentation_assistant_scripts\upload_with_custom_metadata.py "C:\Documentation\Project-A\requirements.md"
```

**Single File Upload** (Development/Testing):

```bash
# Upload individual file
python src\document_upload\common_scripts\upload_single_file.py "C:\Documentation\Project-A\requirements.md"
```

### ğŸ—‘ï¸ Document Management & Cleanup

**Context-Based Deletion** (Targeted Cleanup):

```bash
# Delete all documents for specific context (with confirmation)
python src\document_upload\personal_documentation_assistant_scripts\delete_by_context_and_filename.py "Project-A"

# Delete without confirmation prompt
python src\document_upload\personal_documentation_assistant_scripts\delete_by_context_and_filename.py "Project-A" --no-confirm
```

**File Pattern Deletion** (Flexible Cleanup):

```bash
# Delete documents matching specific filename
python src\document_upload\common_scripts\delete_by_file_path.py "outdated_requirements.md"

# Delete using wildcard patterns
python src\document_upload\common_scripts\delete_by_file_path.py "temp_*.md"

# Delete without confirmation
python src\document_upload\common_scripts\delete_by_file_path.py "draft_*.md" --no-confirm
```

### ğŸ—ï¸ Index & System Management

**Index Operations**:

```bash
# Create or recreate search index
python src\document_upload\common_scripts\create_index.py
```

**System Verification**:

```bash
# Comprehensive system health check
python src\document_upload\common_scripts\verify_document_upload_setup.py
```

### ğŸ”„ Common Workflow Patterns

**Initial Setup & Bulk Upload**:

```bash
# 1. Verify system is ready
python src\document_upload\common_scripts\verify_document_upload_setup.py

# 2. Preview what will be processed
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --dry-run

# 3. Perform actual upload
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py
```

**Adding New Context**:

```bash
# Process only the new context
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "New-Project"
```

**Updating Existing Context**:

```bash
# Option 1: Let system detect changes automatically
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Existing-Project"

# Option 2: Force complete reprocessing
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Existing-Project" --force
```

**Cleaning Up Before Reprocessing**:

```bash
# 1. Delete existing documents
python src\document_upload\personal_documentation_assistant_scripts\delete_by_context_and_filename.py "Context-Name" --no-confirm

# 2. Reprocess with fresh upload
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Context-Name"
```

## ğŸ”„ Updating Documentation

The system is designed for **efficient incremental updates** using sophisticated file tracking:

### ğŸ¯ Smart Change Detection

The **DocumentProcessingTracker** automatically detects file changes using:

- **File Path**: Tracks document location
- **File Size**: Detects content additions/deletions
- **Modification Time**: Identifies when files were last edited
- **Signature Hash**: Creates unique fingerprint for each file state

**When you run upload scripts:**

1. âœ… **New files** â†’ Automatically processed
2. âœ… **Modified files** â†’ Automatically reprocessed
3. â­ï¸ **Unchanged files** â†’ Automatically skipped (improves performance)

### ğŸ“ Adding New Documentation

**For new contexts:**

```bash
# 1. Add files to your Documentation directory structure:
#    Documentation/New-Project/requirements.md
#    Documentation/New-Project/implementation.md

# 2. Process the new context
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "New-Project"
```

**For adding files to existing contexts:**

```bash
# Just run regular upload - system detects new files automatically
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Existing-Project"
```

### âœï¸ Updating Existing Documentation

**For minor edits (recommended):**

```bash
# 1. Edit your files in Documentation directory
# 2. Run upload - system detects changes automatically
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py
```

**For major restructuring:**

```bash
# Force complete reprocessing of specific context
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Project-A" --force
```

### ğŸ”„ Force Reprocessing Options

**Targeted Reprocessing** (Recommended for specific updates):

```bash
# Deletes existing documents for context + reprocesses everything
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --force --context "Project-A"
```

**Complete System Refresh** (Use sparingly):

```bash
# âš ï¸  WARNING: Clears ALL tracking data + deletes ALL documents + reprocesses everything
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --reset
```

### ğŸ§¹ Document Cleanup Workflows

**Before reprocessing a context:**

```bash
# 1. Clean up existing documents
python src\document_upload\personal_documentation_assistant_scripts\delete_by_context_and_filename.py "Project-A" --no-confirm

# 2. Reprocess with fresh upload
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Project-A"
```

**Remove outdated files by pattern:**

```bash
# Clean up old temporary or draft files
python src\document_upload\common_scripts\delete_by_file_path.py "temp_*.md" --no-confirm
python src\document_upload\common_scripts\delete_by_file_path.py "draft_*.md" --no-confirm
```

### ğŸ›ï¸ Performance Optimization Tips

1. **Use context targeting** for faster processing:

   ```bash
   python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Specific-Project"
   ```

2. **Preview changes** before processing large datasets:

   ```bash
   python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --dry-run
   ```

3. **Let the tracker work** - avoid unnecessary `--force` operations which bypass change detection

4. **Clean up before bulk operations** to avoid processing outdated content

## ğŸ› Troubleshooting

### ğŸ”§ Environment & Configuration Issues

**"Failed to connect to Azure OpenAI"**

- âœ… Check `AZURE_OPENAI_ENDPOINT` format: `https://your-service.openai.azure.com/`
- âœ… Verify `AZURE_OPENAI_KEY` is valid (32+ characters)
- âœ… Confirm embedding model deployment exists in Azure OpenAI Studio
- âœ… Ensure `EMBEDDING_DEPLOYMENT` matches exact deployment name
- âœ… Test connection: Run verification script for detailed diagnostics

**"Search service connection failed"**

- âœ… Verify `AZURE_SEARCH_SERVICE` is just the service name (not full URL)
- âœ… Check `AZURE_SEARCH_KEY` is the admin key (not query key)
- âœ… Confirm Azure Cognitive Search service is running and accessible
- âœ… Ensure service tier is **Basic or higher** (Free tier lacks vector search)

**"Index not found" / "404 errors"**

- âœ… Run index creation: `python src\document_upload\common_scripts\create_index.py`
- âœ… Check `AZURE_SEARCH_INDEX` environment variable matches created index name
- âœ… Verify Azure Search service has adequate capacity for new indexes

### ğŸ“ Directory & File Issues

**"No documentation found"**

- âœ… Verify `PERSONAL_DOCUMENTATION_ROOT_DIRECTORY` path exists and is accessible
- âœ… Check directory structure: `Documentation\Project-A\file.md` (not flat file structure)
- âœ… Ensure context directories contain supported files (`.md`, `.txt`, `.docx`)
- âœ… Run verification script to see directory scanning results

**"Documents not processing"**

- âœ… Check file permissions (read access required)
- âœ… Verify file encoding (UTF-8 recommended)
- âœ… Ensure files aren't locked by other applications
- âœ… Check file size limits (very large files may cause memory issues)

### ğŸ”„ Processing & Upload Issues

**"Documents not updating after changes"**

- ğŸ” **Check file tracking**: System uses file signature (path + size + mtime)
- âœ… Verify files were actually saved with new modification time
- âœ… Force reprocessing for specific context:
  ```bash
  python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Project-A" --force
  ```
- âœ… Clear tracking data if necessary:
  ```bash
  python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --reset
  ```

**"Vector search not supported"**

- âœ… Upgrade Azure Cognitive Search to **Basic tier or higher**
- âœ… Free tier doesn't support vector search capabilities
- âœ… Recreate index after tier upgrade

**"Memory errors during processing"**

- âœ… Process smaller batches using `--context` targeting
- âœ… System uses generators for O(1) memory complexity, but very large files can still cause issues
- âœ… Split large documents into smaller files
- âœ… Ensure adequate system RAM (4GB+ recommended for large document sets)

### ğŸ“Š Index & Search Issues

**"Inconsistent document counts"**

- ğŸ§¹ Clean up orphaned documents:
  ```bash
  python src\document_upload\personal_documentation_assistant_scripts\delete_by_context_and_filename.py "Project-A" --no-confirm
  ```
- âœ… Reprocess with clean upload:
  ```bash
  python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Project-A"
  ```

**"Search returns no results"**

- âœ… Verify documents were actually indexed (check document count)
- âœ… Test with broader search terms
- âœ… Check that search index contains expected context names
- âœ… Try different search modes (text, vector, hybrid)

**"Duplicate documents appearing"**

- ğŸ§¹ Clear specific context and reprocess:
  ```bash
  python src\document_upload\personal_documentation_assistant_scripts\delete_by_context_and_filename.py "Project-A" --no-confirm
  python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Project-A"
  ```

### ğŸ› ï¸ Diagnostic Commands

**Run comprehensive diagnostics:**

```bash
python src\document_upload\common_scripts\verify_document_upload_setup.py
```

**Check specific context processing:**

```bash
python src\document_upload\personal_documentation_assistant_scripts\upload_with_pipeline.py --context "Project-A" --dry-run
```

**Test search functionality:**

```bash
python -c "
import sys; sys.path.append('src')
from mcp_server.tools.universal_tools import UniversalDocumentationTools
import asyncio
async def test():
    tools = UniversalDocumentationTools()
    results = await tools.search_documents('test', 'text', max_results=1)
    print(f'Search test: {len(results)} results')
asyncio.run(test())
"
```

## âœ… Success Checklist

- [ ] Azure OpenAI service created and embedding model deployed
- [ ] Azure Cognitive Search service created (Basic tier+)
- [ ] Environment variables configured in `.env`
- [ ] `python verify_document_upload_setup.py` passes all checks
- [ ] Search index created successfully
- [ ] Documents uploaded and indexed
- [ ] Test searches return relevant results

## ğŸ”— Next Steps

Once document upload is complete, you can set up the **MCP Server component**:

- See [MCP_SERVER_SETUP.md](MCP_SERVER_SETUP.md) for VS Code integration

---

**ğŸ“„ Document Upload System is now ready!** Your documentation is indexed and searchable.
